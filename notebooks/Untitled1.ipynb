{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3125dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\WorkFolder\\nlp_thes_augm\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3871a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nlp_thes_augm.utils.wordnet import RuWordNet\n",
    "from nlp_thes_augm.models.augmentation import WordnetAugmentator\n",
    "\n",
    "wordnet_path = r'D:\\WorkFolder\\data\\models\\RuWordNet'\n",
    "wordnet = RuWordNet(wordnet_path)\n",
    "\n",
    "t5_model_path = r'D:\\WorkFolder\\data\\models\\rut5_base_restorer_1m'\n",
    "gpt2_model_path = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "wordnet_augmentator = WordnetAugmentator(wordnet, t5_model_path, gpt2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb6afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 116.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В Совфеде при президенте России пообсуждали важнейший проект по освоению российской Арктики, президент Российской Федерации согласен с позицией.\n",
      "45.1832389831543\n",
      "В Совфеде при президенте РФ обговорили важнейшую реализацию проекта по освоению российской Арктики, глава российского государства согласен с позицией.\n",
      "51.7611083984375\n",
      "В Совфеде при президенте РФ обсудили важнейший проект по освоению российской Арктики, президент РФ согласен с позицией воинской части.\n",
      "52.104129791259766\n",
      "В Совфеде при президенте России обсудили важный проект по освоению общероссийской Арктики, президент РФ согласен с положениями.\n",
      "53.42341232299805\n",
      "В Совфеде при президенте России пообсуждали важнейший проект по освоению российского Арктического района, президент РФ согласен с положениями.\n",
      "53.84498596191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'В Совфеде при президент рф обсудили важнейший проект по освоению российской Арктики, президент рф согласен с позицией.'\n",
    "augmentations, _ = wordnet_augmentator.augmentate(\n",
    "    text, augmentations_count=100, topk=5, max_concept_count=5, bs=100\n",
    ")\n",
    "for augm in augmentations:\n",
    "    print(augm.restored_augmentation)\n",
    "    print(augm.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.restored_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.concepts[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.restored_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_augmentator.concept_detector.detect(augm.restored_augmentation)[0].get_sense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd8d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "augm_text_tokenized = re.findall(wordnet_augmentator.text_preprocessor.sep_reg, augm.restored_augmentation)\n",
    "augm_concepts = augm.concepts\n",
    "augm_senses = augm.senses\n",
    "augm_synsets_ids = augm.synsets_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea8125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markup_augmentation(wordnet_augmentator, wordnet, augm):\n",
    "    augm_text_tokenized = re.findall(wordnet_augmentator.text_preprocessor.sep_reg, augm.restored_augmentation)\n",
    "    augm_concepts = augm.concepts\n",
    "    augm_senses = augm.senses\n",
    "    augm_synsets_ids = augm.synsets_ids\n",
    "\n",
    "    current_concept_idx = 0\n",
    "    current_concept_num = augm_concepts[current_concept_idx].tokens[0].num\n",
    "    shift = 0\n",
    "    counter = 0\n",
    "\n",
    "    result_markup_text = ''\n",
    "    for i, token in enumerate(augm_text_tokenized):\n",
    "        if counter > 0:\n",
    "            result_markup_text += ' ' + token\n",
    "            if counter == 1:\n",
    "                result_markup_text += '</u>'\n",
    "            counter -= 1\n",
    "            continue\n",
    "        if i == current_concept_num + shift:\n",
    "\n",
    "            selected_synset_name = wordnet.synsets[augm_synsets_ids[current_concept_idx]].synset_name\n",
    "            selected_sense =  augm_senses[current_concept_idx]\n",
    "\n",
    "            hover_text = f'<b>Выбранный концепт:</b><br>{selected_synset_name}<br><b>Выбранный текстовый вход:</b><br>{selected_sense}'\n",
    "            open_tag = f'<u data-toggle=\\\"tooltip\\\" data-html=\\\"true\\\" title=\\\"{hover_text}\\\">'\n",
    "            result_markup_text += open_tag\n",
    "\n",
    "            counter = len(augm_senses[current_concept_idx].split('_'))\n",
    "            shift += counter - len(augm_concepts[current_concept_idx].tokens)\n",
    "            current_concept_idx += 1\n",
    "            current_concept_num = augm_concepts[current_concept_idx].tokens[0].num if current_concept_idx < len(augm_concepts) else len(augm_text_tokenized)\n",
    "\n",
    "        result_markup_text += ' ' + token\n",
    "        if counter == 1:\n",
    "            result_markup_text += '</u>'\n",
    "        counter -= 1\n",
    "    return result_markup_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2584aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В Совфеде при президенте РФ<u data-toggle=\"tooltip\" data-html=\"true\" title=\"<b>Выбранный концепт:</b><br>обсуждение<br><b>Выбранный текстовый вход:</b><br>обговорить\"> обговорили</u> важнейшую<u data-toggle=\"tooltip\" data-html=\"true\" title=\"<b>Выбранный концепт:</b><br>проект (комплекс взаимосвязанных работ)<br><b>Выбранный текстовый вход:</b><br>реализация_проект\"> реализацию проекта</u> по освоению российской<u data-toggle=\"tooltip\" data-html=\"true\" title=\"<b>Выбранный концепт:</b><br>арктика<br><b>Выбранный текстовый вход:</b><br>арктика\"> Арктики</u> ,<u data-toggle=\"tooltip\" data-html=\"true\" title=\"<b>Выбранный концепт:</b><br>президент россии<br><b>Выбранный текстовый вход:</b><br>глава_российский_государство\"> глава российского государства</u><u data-toggle=\"tooltip\" data-html=\"true\" title=\"<b>Выбранный концепт:</b><br>согласный звук<br><b>Выбранный текстовый вход:</b><br>согласный\"> согласен</u> с позицией .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup_augmentation(wordnet_augmentator, wordnet, augmentations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(augm_concepts[0].synsets_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm_concepts = augm.concepts\n",
    "augm_senses = augm.senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a27f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.concepts[0].tokens[0].num, len(augm.concepts[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5217d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augm.senses[0].split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.sense2synid['президент_рф']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ec19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augm.restored_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_thes_augm.utils.common import inject_concept_tokens\n",
    "inject_concept_tokens(augm.concepts, augm.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b659ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from functools import lru_cache\n",
    "import string \n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, word, token, num, shift):\n",
    "        self.word = word\n",
    "        self.word_len = len(word)\n",
    "        self.token = token\n",
    "        self.num = num\n",
    "        self.shift = shift\n",
    "        \n",
    "class ConceptToken:\n",
    "    def __init__(self, tokens, synsets_ids):\n",
    "        self.tokens = tokens\n",
    "        self.synsets_ids = synsets_ids\n",
    "        \n",
    "    def get_sense(self):\n",
    "        return ' '.join([t.word for t in self.tokens])\n",
    "    \n",
    "    def get_sense_norm(self):\n",
    "        return '_'.join([t.token for t in self.tokens])\n",
    "        \n",
    "    def __str__(self):\n",
    "        sense = self.get_sense()\n",
    "        sense_norm = self.get_sense_norm()\n",
    "        synsets = ','.join(self.synsets_ids)\n",
    "        return '{{' + sense + '|' + sense_norm + '|' + synsets + '}}'\n",
    "        \n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.sep_reg = re.compile(r\"[\\w]+|[!\\\"#$%&\\'()*+,-–—./:;<=>?@\\[\\\\\\]^_`{|}~“”«»]\")\n",
    "        self.morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "    @lru_cache(maxsize=1000000)\n",
    "    def get_normal_form(self, word):\n",
    "        return self.morph_analyzer.parse(word)[0].normal_form\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        text_splitted = re.findall(self.sep_reg, text)\n",
    "        shift = 0\n",
    "        result = []\n",
    "        for i, w in enumerate(text_splitted):\n",
    "            w_norm = self._normalize_word(w)\n",
    "            if text[shift] == ' ':\n",
    "                shift += 1\n",
    "            w_shift = shift\n",
    "            shift = shift + len(w)\n",
    "            result.append(Token(w, w_norm, i, w_shift))\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _normalize_word(self, word):\n",
    "        if (not word.isalpha()) or (len(word) < 3) or word.isupper():\n",
    "            return word\n",
    "\n",
    "        return self.get_normal_form(word).lower()\n",
    "\n",
    "class ConceptDetector:\n",
    "    def __init__(self, wordnet, text_preprocessor):\n",
    "        self.wordnet = wordnet\n",
    "        self.senses = {}\n",
    "        self.issensecontinuation = set()\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self._transform_senses()\n",
    "\n",
    "    def _transform_senses(self):\n",
    "        for sense in self.wordnet.senses:\n",
    "            splited_sense = sense.split('_')\n",
    "            sense_len = len(splited_sense)\n",
    "            for i in range(1, sense_len):\n",
    "                self.issensecontinuation.add('_'.join(splited_sense[:i]))\n",
    "\n",
    "            if sense_len not in self.senses:\n",
    "                self.senses[sense_len] = set()\n",
    "\n",
    "            self.senses[sense_len].add(sense)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _filter_tokens(tokens):\n",
    "        return [token for token in tokens if token.token.isalpha() and not token.token.isupper()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sanity_check(tokens, text):\n",
    "        for token in tokens:\n",
    "            if token.word != text[token.shift:token.shift + token.word_len]:\n",
    "                raise Exception(f'ERROR: {token.word} != {text[token.shift:token.shift + token.word_len]}')\n",
    "                \n",
    "    def detect(self, text):\n",
    "        tokens = self._filter_tokens(self.text_preprocessor.tokenize_text(text))\n",
    "        self._sanity_check(tokens, text)\n",
    "        concepts = []\n",
    "\n",
    "        start_w_i = 0\n",
    "        while start_w_i < len(tokens):\n",
    "            last_ok_tokens = [tokens[start_w_i]]\n",
    "            for end_w_i in range(start_w_i + 1, len(tokens) + 1):\n",
    "                if end_w_i - start_w_i > 1 and tokens[end_w_i-1].num - tokens[end_w_i-2].num != 1:\n",
    "                    break\n",
    "                word = '_'.join([t.token for t in tokens[start_w_i:end_w_i]])\n",
    "                if word in self.senses[end_w_i - start_w_i]:\n",
    "                    if end_w_i - start_w_i > 1:\n",
    "                        last_ok_tokens = tokens[start_w_i:end_w_i]\n",
    "\n",
    "                if word not in self.issensecontinuation:\n",
    "                    break\n",
    "            if '_'.join([t.token for t in last_ok_tokens]) in self.senses[len(last_ok_tokens)]:\n",
    "                concepts.append(last_ok_tokens)\n",
    "            start_w_i += len(last_ok_tokens)\n",
    "        \n",
    "        concepts = [ConceptToken(concept_tokens, self.wordnet.sense2synid['_'.join([t.token for t in concept_tokens])]) for concept_tokens in concepts]\n",
    "        return concepts\n",
    "    \n",
    "class RandomConceptsSubsetSelector:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select(self, concepts, max_concepts_count=5, only_deterministic=False):\n",
    "        if only_deterministic:\n",
    "            concepts = [c for c in concepts if len(c.synsets_ids) == 1]\n",
    "        concept_count = min(max_concepts_count, len(concepts))\n",
    "        if concept_count == 0:\n",
    "            return []\n",
    "        \n",
    "        idx = np.random.choice(range(len(concepts)), concept_count, replace=False)\n",
    "        selected_concepts = np.array(concepts)[idx].tolist()\n",
    "        return sorted(selected_concepts, key=lambda x: x.tokens[0].num)\n",
    "    \n",
    "class RandomConceptSelector:\n",
    "    def __init__(self, wordnet):\n",
    "        self.wordnet = wordnet\n",
    "    \n",
    "    def select(self, concept):\n",
    "        sense = concept.get_sense_norm()\n",
    "        synsets_ids = self.wordnet.sense2synid[sense]\n",
    "        return np.random.choice(synsets_ids, 1).tolist()[0]\n",
    "    \n",
    "class RandomConceptSenseSelector:\n",
    "    def __init__(self, wordnet):\n",
    "        self.wordnet = wordnet\n",
    "        \n",
    "    def select(self, synset_id):\n",
    "        senses = list(self.wordnet.synsets[synset_id].synset_words)\n",
    "        idx = np.random.choice(range(len(senses)), 1).tolist()[0]\n",
    "        return senses[idx]\n",
    "    \n",
    "class Augmentation:\n",
    "    def __init__(self, text, concepts, senses, augmentation, restored_augmentation='', score =1.0):\n",
    "        self.text = text\n",
    "        self.concepts = concepts\n",
    "        self.senses = senses\n",
    "        self.augmentation = augmentation\n",
    "        self.restored_augmentation = restored_augmentation\n",
    "        self.score = score\n",
    "\n",
    "class ConceptAugment:\n",
    "    def __init__(self, wordnet, concept_subset_selector, concept_selector, sense_selector):\n",
    "        self.wordnet = wordnet\n",
    "        self.concept_subset_selector = concept_subset_selector\n",
    "        self.concept_selector = concept_selector\n",
    "        self.sense_selector = sense_selector\n",
    "        \n",
    "    def augment_one(self, concepts, text, max_concepts_count, only_deterministic=False):\n",
    "        concepts_subset = self.concept_subset_selector.select(\n",
    "            copy.deepcopy(concepts), max_concepts_count=max_concepts_count, \n",
    "            only_deterministic=only_deterministic\n",
    "        )\n",
    "        synset_ids = [self.concept_selector.select(c) for c in concepts_subset]\n",
    "        senses = [self.sense_selector.select(synid) for synid in synset_ids]\n",
    "        \n",
    "        return inject_augmentation(concepts_subset, senses, text), concepts_subset, senses\n",
    "    \n",
    "    def augmentate(self, concepts, text, max_concepts_count, augmentations_count, only_deterministic=False):\n",
    "        augmentations = []\n",
    "        for i in range(augmentations_count):\n",
    "            augmentation, concepts_subset, senses = self.augment_one(\n",
    "                concepts, text, max_concepts_count, only_deterministic\n",
    "            )\n",
    "            augmentations.append(Augmentation(text, concepts_subset, senses, augmentation))\n",
    "            \n",
    "        return augmentations\n",
    "    \n",
    "class T5TextRestore:\n",
    "    def __init__(self, model_path, device='cuda', prefix='thes_augm: '):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.prefix = prefix\n",
    "        \n",
    "    def restore(self, text):\n",
    "        text = self.prefix + text\n",
    "        with torch.no_grad():\n",
    "            model_input = self.tokenizer(text, return_tensors='pt', padding=True).to(self.device)\n",
    "            max_size = int(model_input.input_ids.shape[1] * 1.5 + 10)\n",
    "            out = self.model.generate(**model_input, max_length=max_size).detach().cpu()[0]\n",
    "            result = self.tokenizer.decode(out, skip_special_tokens=True)\n",
    "        return result\n",
    "    \n",
    "    def restore_batch(self, texts, bs=4):\n",
    "        texts = [self.prefix + t for t in texts]\n",
    "        batch_count = len(texts) // bs + int(len(texts) % bs != 0)\n",
    "        results = []\n",
    "        for i in tqdm(range(batch_count)):\n",
    "            batch = texts[i * bs: (i + 1) * bs]\n",
    "            model_input = self.tokenizer(batch, return_tensors='pt', padding=True).to(self.device)\n",
    "            max_size = int(model_input.input_ids.shape[1] * 1.5 + 10)\n",
    "            out = self.model.generate(**model_input, max_length=max_size).detach().cpu()\n",
    "            result = [self.tokenizer.decode(o, skip_special_tokens=True) for o in out]\n",
    "            results += result\n",
    "        return results\n",
    "        \n",
    "class GPT2Score:\n",
    "    def __init__(self, model_path, device='cuda'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        self.device = device\n",
    "        \n",
    "    def score(self, text):\n",
    "        max_length = self.model.config.n_positions\n",
    "        stride = 512\n",
    "        encodings = self.tokenizer(text, return_tensors='pt')\n",
    "        nlls = []\n",
    "        for i in range(0, encodings.input_ids.size(1), stride):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - i    # may be different from stride on last loop\n",
    "            input_ids = encodings.input_ids[:,begin_loc:end_loc].to(self.device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:,:-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        return float(ppl.detach().cpu())\n",
    "    \n",
    "class WordnetAugmentator:\n",
    "    def __init__(self, wordnet, t5_model_path, gpt2_model_path):\n",
    "        self.wordnet = wordnet\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.concept_detector = ConceptDetector(wordnet, self.text_preprocessor)\n",
    "        \n",
    "        concepts_subset_selector = RandomConceptsSubsetSelector()\n",
    "        concept_selector = RandomConceptSelector(wordnet)\n",
    "        sense_selector = RandomConceptSenseSelector(wordnet)\n",
    "        self.concept_augmentator = ConceptAugment(wordnet, concepts_subset_selector, concept_selector, sense_selector)\n",
    "        \n",
    "        self.t5_text_resorer = T5TextRestore(t5_model_path)\n",
    "        self.gpt2_score = GPT2Score(gpt2_model_path)\n",
    "        \n",
    "    def augmentate(self, text, augmentations_count, topk, max_concept_count=5, bs=1):\n",
    "        detected_concepts = self.concept_detector.detect(text)\n",
    "        augmentations = self.concept_augmentator.augmentate(\n",
    "            detected_concepts, text, max_concept_count, augmentations_count\n",
    "        )\n",
    "        \n",
    "        if bs > 1:\n",
    "            restored_augmentations = self.t5_text_resorer.restore_batch(\n",
    "                [augm.augmentation for augm in tqdm(augmentations)], bs=bs\n",
    "            )\n",
    "        else:\n",
    "            restored_augmentations = []\n",
    "            for augm in tqdm(augmentations):\n",
    "                restored_augmentations.append(self.t5_text_resorer.restore(augm.augmentation))\n",
    "                \n",
    "        for i in range(len(restored_augmentations)):\n",
    "            augmentations[i].restored_augmentation = restored_augmentations[i]\n",
    "            \n",
    "        for augm in tqdm(augmentations):\n",
    "            augm.score = self.gpt2_score.score(augm.restored_augmentation)\n",
    "            \n",
    "        augmentations = sorted(augmentations, key=lambda x: x.score)\n",
    "        augmentations = self._filter_duplicates(augmentations)\n",
    "        return augmentations[:topk]\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_duplicates(augmentations):\n",
    "        processed = set()\n",
    "        filtered_augmentations = []\n",
    "        for augm in augmentations:\n",
    "            if augm.text == augm.restored_augmentation:\n",
    "                continue\n",
    "            if augm.restored_augmentation in processed:\n",
    "                continue\n",
    "            filtered_augmentations.append(augm)\n",
    "            processed.add(augm.restored_augmentation)\n",
    "            \n",
    "        return filtered_augmentations\n",
    "    \n",
    "#====================================\n",
    "def inject_concept_tokens(concept_tokens, text):\n",
    "    shift = 0\n",
    "    text_injected = ''\n",
    "    for concept_token in concept_tokens:\n",
    "        first_token = concept_token.tokens[0]\n",
    "        last_token = concept_token.tokens[-1]\n",
    "\n",
    "        info = str(concept_token)\n",
    "        text_injected += text[shift:first_token.shift] + info\n",
    "\n",
    "        shift = last_token.shift + last_token.word_len\n",
    "\n",
    "    text_injected += text[shift:]\n",
    "    return text_injected\n",
    "    \n",
    "def inject_augmentation(concept_tokens, senses, text):\n",
    "    text_augmented = ''\n",
    "    shift = 0\n",
    "\n",
    "    for concept, sense in zip(*[concept_tokens, senses]):\n",
    "        sense = sense.replace('_', ' ')\n",
    "\n",
    "        first_token = concept.tokens[0]\n",
    "        last_token = concept.tokens[-1]\n",
    "\n",
    "        text_augmented += text[shift:first_token.shift] + f'{sense}'\n",
    "        shift = last_token.shift + last_token.word_len\n",
    "\n",
    "    text_augmented += text[shift:]\n",
    "    return text_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bfbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_path = r'D:\\WorkFolder\\data\\t5_augm\\rut5_base_restorer_1m'\n",
    "gpt2_model_path = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "wordnet_augmentator = WordnetAugmentator(wordnet, t5_model_path, gpt2_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text = 'Ее выполнение позволит правительственным войскам САР установить контроль над сирийско-турецкой границей.'\n",
    "augmentations = wordnet_augmentator.augmentate(text, 50, 5, bs=10)\n",
    "for augm in augmentations:\n",
    "    print(augm.restored_augmentation)\n",
    "    print(augm.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18039459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(r'D:\\WorkFolder\\data\\augmentation_data_news2017\\0_sentences_test.csv')\n",
    "test['text'] = test['text'].apply(lambda x: x.replace('\\xa0', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74932242",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in test['text']:\n",
    "    augmentations = wordnet_augmentator.augmentate(text, 50, 5, bs=10)\n",
    "    for augm in augmentations:\n",
    "        print(augm.restored_augmentation)\n",
    "        print(augm.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a948dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = test.sample(4)['corrupted_text'].tolist()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72200a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_batch(\n",
    "    texts, \n",
    "    wordnet_augmentator.t5_text_resorer.model, \n",
    "    wordnet_augmentator.t5_text_resorer.tokenizer, \n",
    "    wordnet_augmentator.t5_text_resorer.prefix, \n",
    "    wordnet_augmentator.t5_text_resorer.device,\n",
    "    max_bs = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220de893",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Ее выполнение позволит правительственным войскам САР установить контроль над сирийско-турецкой границей.'\n",
    "for res in wordnet_augmentator.augmentate(text, 50, 5):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print('---'*10)\n",
    "print(inject_concept_tokens(detected_concepts, text))\n",
    "print('---'*10)\n",
    "for i in range(5):\n",
    "    augmented_text = augmentator.augment_one(detected_concepts, text)\n",
    "    restored_augmented_text = t5_text_resorer.restore(augmented_text)\n",
    "    score = gpt2_score.score(restored_augmented_text)\n",
    "    print(augmented_text)\n",
    "    print(restored_augmented_text)\n",
    "    print(score)\n",
    "    print('---'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1721ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets['673-N'].synset_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_tokens(detected_concepts, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.sense2synid['турецкий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Словестная'\n",
    "%timeit text_preprocessor.morph_analyzer.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'многие-многие'\n",
    "text_preprocessor.get_morph(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(*[[1], [2]]):\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socket import gethostbyaddr, getfqdn, gethostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ede52",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gethostbyaddr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23155d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gethostbyaddr('127.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'В Совфеде {{обсудили|обсудить|106800-V}} {{важнейший|важный|119261-A,112920-A}} {{проект|проект|106466-N,7059-N,112123-N,138540-N}} по {{освоению|освоение|130167-N,106685-N}} {{российской|российский|2636-A}} {{Арктики|арктика|105540-N}}.'\n",
    "text = text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.findall(r'{{(.*?)}}', text[3])[0].split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de33d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
