{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e190ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e17378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "archive = zipfile.ZipFile(r'D:\\WorkFolder\\data\\news2017_raw\\news2017\\201701\\01012017-000000-R517858803_htm.zip', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f34cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_names = archive.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "data = io.TextIOWrapper(archive.open(files_names[1]), 'windows-1251').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd272c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "news_dir = r'D:\\WorkFolder\\data\\news2017_raw\\news2017'\n",
    "news_out_dir = r'D:\\WorkFolder\\data\\news2017'\n",
    "for i, folder in enumerate(os.listdir(news_dir)):\n",
    "    df = pd.DataFrame()\n",
    "    documents = []\n",
    "    \n",
    "    news_subdir = os.path.join(news_dir, folder)\n",
    "    for zip_file_name in tqdm(os.listdir(news_subdir)):\n",
    "        archive_path = os.path.join(news_subdir, zip_file_name)\n",
    "        archive = zipfile.ZipFile(archive_path, 'r')\n",
    "        \n",
    "        for file_name in archive.namelist():\n",
    "            try:\n",
    "                data = io.TextIOWrapper(archive.open(file_name), 'windows-1251').read()\n",
    "            except:\n",
    "                continue\n",
    "            documents.append(preprocess_htm(data))\n",
    "    df['text'] = documents\n",
    "    df.to_csv(os.path.join(news_out_dir, f'{i}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be45581",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec665d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048529f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_htm(text):\n",
    "    text = text.replace('&lt;', '<').replace('&gt;', '>')\n",
    "    text = re.sub(r'<HTML>(.*?)</NOMORPH>', '', text, flags=re.DOTALL|re.M)\n",
    "    if '</BODY>' in text:\n",
    "        text = text[:text.find('</BODY>')]\n",
    "    text = re.sub(r'<(.*?)>', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_htm(test_df.iloc[2].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_text'] = test_df['raw_htm'].apply(lambda x: preprocess_htm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d29368",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df['clean_text'].sample(1).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9900e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test_df['clean_text'].sample(1).tolist()[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827d00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from functools import lru_cache\n",
    "\n",
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "@lru_cache(maxsize=1000000)\n",
    "def get_morph(word):\n",
    "    return morph_analyzer.parse(word)[0].normal_form\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    if word.isdigit() or len(word) < 3:\n",
    "        return word\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    if word[0] in string.punctuation:\n",
    "        prefix = word[0]\n",
    "        word = word[1:]\n",
    "        \n",
    "    if word[-1] in string.punctuation:\n",
    "        suffix = word[-1]\n",
    "        word = word[:-1]\n",
    "    \n",
    "    return prefix + get_morph(word) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e8668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_text(text, frac=0.2):\n",
    "    splitted_text = text.split()\n",
    "    seeds = sorted(np.random.choice(range(0, len(splitted_text)), len(splitted_text) // int(1 / frac) + 1, replace=False))\n",
    "    for i, seed in enumerate(seeds):\n",
    "        n = min(3, seeds[i + 1] - seed if i < len(seeds) - 1 else len(splitted_text) - seed)\n",
    "        for j in range(seed, seed + n):\n",
    "            splitted_text[j] = lemmatize_word(splitted_text[j])\n",
    "    return ' '.join(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc337e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd699d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b0c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88d55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    if word.isdigit() or len(word) < 3:\n",
    "        return word\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    if word[0] in string.punctuation:\n",
    "        prefix = word[0]\n",
    "        word = word[1:]\n",
    "        \n",
    "    if word[-1] in string.punctuation:\n",
    "        suffix = word[-1]\n",
    "        word = word[:-1]\n",
    "    \n",
    "    return prefix + get_morph(word) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa80d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df_path = r'D:\\WorkFolder\\data\\news2017\\0.csv'\n",
    "df = pd.read_csv(df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccf78b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7189f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 1/855211 [00:00<2:36:46, 90.92it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4edb5d0d14a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corrupted_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrupt_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    812\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1c0e68a01086>\u001b[0m in \u001b[0;36mcorrupt_text\u001b[1;34m(text, frac)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorrupt_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msplitted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitted_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitted_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfrac\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitted_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['corrupted_text'] = df['text'].progress_apply(corrupt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc250fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(df):\n",
    "    sample = df.sample(1)\n",
    "    print(sample['text'].tolist()[0])\n",
    "    print(sample['corrupted_text'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467fc371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3d3d34aa0d4c3fbffc366d39f475f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/828k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2df10f7bdf94158bee51ae79d7e548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f19255d50724feabfca296fcdfd1980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/315 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все охотники хотят знать где фазан сидит.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model.cuda();\n",
    "model.eval();\n",
    "\n",
    "def paraphrase(text, beams=5, grams=4, do_sample=False):\n",
    "    x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\n",
    "    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
    "    out = model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(paraphrase('Каждый охотник желает знать, где сидит фазан.'))\n",
    "# Все охотники хотят знать где фазан сидит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa7779c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78a8874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Многие данные имеют графическую природу, например, социальные графические графы, переходы по ссылке, графические знания, таксономия и т.д. В этом случае встает вопрос о том, как учитывать информацию связей между вершинами при построении графических векторных изображений.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(some_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a0456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a390b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method synsets in module nltk.corpus.reader.wordnet:\n",
      "\n",
      "synsets(lemma, pos=None, lang='eng', check_exceptions=True) method of nltk.corpus.reader.wordnet.WordNetCorpusReader instance\n",
      "    Load all synsets with a given lemma and part of speech tag.\n",
      "    If no pos is specified, all synsets for all parts of speech\n",
      "    will be loaded.\n",
      "    If lang is specified, all the synsets associated with the lemma name\n",
      "    of that language will be returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wordnet.synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92fac230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('walk.n.01')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('walking', wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af68766a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('walk.n.01'),\n",
       " Synset('base_on_balls.n.01'),\n",
       " Synset('walk.n.03'),\n",
       " Synset('walk.n.04'),\n",
       " Synset('walk.n.05'),\n",
       " Synset('walk.n.06'),\n",
       " Synset('walk_of_life.n.01'),\n",
       " Synset('walk.v.01'),\n",
       " Synset('walk.v.02'),\n",
       " Synset('walk.v.03'),\n",
       " Synset('walk.v.04'),\n",
       " Synset('walk.v.05'),\n",
       " Synset('walk.v.06'),\n",
       " Synset('walk.v.07'),\n",
       " Synset('walk.v.08'),\n",
       " Synset('walk.v.09'),\n",
       " Synset('walk.v.10')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00295aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have', 'have_got', 'hold']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('had')[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2d198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88ab7d1f138adb178a7ddf734a0a1dd2d25db2780a6169a21e3b8f8b839a01a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
